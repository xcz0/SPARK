# 模型详细建模方案：Time-Aware Multi-Head Causal Transformer

## 1. 总体架构设计 (High-Level Architecture)

模型采用 **Decoder-only Transformer** 结构。核心改进在于放弃了通用的全局 Attention，转而使用**多头差异化掩码机制 (Multi-Head Differential Masking)**，强制不同的注意力头（Heads）关注序列中的不同维度（单一卡片历史、同知识点关联、全局学习状态）。

*   **输入序列**: 用户的一次交互序列 $S = \{x_1, x_2, ..., x_L\}$。
*   **上下文窗口**: 固定窗口大小（如 $L=256$）。
*   **时间建模**: 全量替换离散位置编码，使用连续时间编码 (Time2Vec)。


## 2. 输入层重构 (Input Layer Design)

输入向量 $E_t$ 不再包含离散的位置 Embedding，而是由**语义特征**和**时间特征**动态融合而成。

### A. 特征流处理

#### 1. 数值特征流 (Numerical Stream)
包含原本的数值特征以及新增的计数特征。
*   **特征列表**:
    *   `log_elapsed_seconds`, `log_elapsed_days` (间隔)
    *   `log_cumulative` (累计学习量)
    *   `prev_log_duration`, `last_log_duration_on_card` (耗时历史)
    *   `card_index_today`: 当日复习的第几张卡片。
*   **编码方式**:
    *   将所有数值特征拼接为向量 $v_{num} \in \mathbb{R}^{d_{in}}$。
    *   通过 MLP 投影: $h_{num} = \text{Linear}(v_{num}) \rightarrow \mathbb{R}^{d_{model}/2}$。

#### 2. 类别特征流 (Categorical Stream)
*   **特征列表**: `state`, `rating` (shifted `prev_rating`), `is_first_review` 等。
*   **编码方式**: Embedding 查表后拼接，投影至 $h_{cat} \in \mathbb{R}^{d_{model}/2}$。

### B. 连续时间编码 (Time2Vec)
使用可学习的正弦波函数捕获周期性和线性时间模式，替代 Transformer 的 Positional Encoding。

对于每一个时间步 $t$，提取其**全局绝对时间戳**$T_t$。
Time2Vec 模块 $T2V(\tau)$ 定义如下：

$$ \text{T2V}(\tau)[k] = \begin{cases} \omega_k \tau + \phi_k & \text{if } k = 0 \text{ (线性项, 捕捉长期趋势)} \\ \sin(\omega_k \tau + \phi_k) & \text{if } 1 \le k < K \text{ (周期项, 捕捉遗忘/复习周期)} \end{cases} $$

*   **输入**: $T_t$ (Current Global Timestamp)。
*   **参数**: $\omega \in \mathbb{R}^K, \phi \in \mathbb{R}^K$ 为可学习参数。
*   **输出**: $h_{time} \in \mathbb{R}^K$ (通常设 $K=d_{model}$)。
*   **作用**: 使模型能够理解“20分钟前”和“3天前”在连续统上的区别，而不仅仅是序列索引的区别。

### C. 最终输入融合
$$ E_t = \text{LayerNorm}(\text{Concat}[h_{num}, h_{cat}]) + h_{time} $$
*(注：这里采用加法融合 Time2Vec，类似于位置编码的用法)*

## 3. 多头差异化注意力机制 (Multi-Head Differential Attention)

这是模型的核心。我们将 $H$ 个注意力头（Heads）划分为三组，每组应用不同的 **Attention Mask** 和 **Bias** 策略，以解耦关注点。

假设 $d_{model}=512, H=8$，则每组分配若干个 Head。

### 公式通式
对于第 $h$ 个头，注意力计算为：
$$ \text{Attn}_h(Q, K, V) = \text{Softmax}\left( \frac{Q_h K_h^T}{\sqrt{d_k}} + \mathbf{M}^{(h)} \right) V_h $$

其中 $\mathbf{M}^{(h)}$ 是一个 $(L, L)$ 的矩阵，$M_{i,j}$ 表示第 $i$ 步对第 $j$ 步的关注偏置。

### 分组策略设计

#### 组 A: 卡片记忆回溯头 (Card-Specific Heads)
*   **分配**: 4 个 Head。
*   **目标**: 强迫模型只看**同一张卡片**的历史，并模拟记忆衰减。
*   **掩码构成 ($M^{(card)}_{i,j}$)**:
    1.  **Causal Mask**: $j > i \rightarrow -\infty$。
    2.  **Identity Mask (Hard)**: 若 $\text{card\_id}_i \neq \text{card\_id}_j$，则 $M_{i,j} = -\infty$。
        *   *作用*: 彻底屏蔽其他卡片的干扰，实现纯粹的“单卡时间序列”提取。
    3.  **Time-Decay Bias (Soft)**:
        $$ \text{Bias}_{decay}(i, j) = w_{card} \cdot \exp\left( -\lambda \cdot |T_i - T_j| \right) $$
        *   $T$: 全局时间戳（单位：天或小时）。
        *   $w_{card}$: 可学习的标量权重（控制该头对时间的敏感度）。
        *   $\lambda$: 可学习的衰减系数（必须 $>0$，通过 Softplus 约束）。
        *   *作用*: 距离当前复习越近的历史记录，权重越高（符合 Recency Effect）。

#### 组 B: 知识关联头 (Concept/Deck Heads)
*   **分配**: 2 个 Head。
*   **目标**: 捕捉同一卡组（Deck）下的知识迁移或干扰。
*   **掩码构成 ($M^{(deck)}_{i,j}$)**:
    1.  **Causal Mask**: $j > i \rightarrow -\infty$。
    2.  **Concept Mask (Hard)**: 若 $\text{deck\_id}_i \neq \text{deck\_id}_j$，则 $M_{i,j} = -\infty$。
    3.  **Self-Exclusion**: 若 $\text{card\_id}_i == \text{card\_id}_j$，则 $M_{i,j} = -\infty$。
        *   *作用*: 排除自己（由组A负责），只看同组的**其他**卡片。

#### 组 C: 全局上下文头 (Global Context Heads)
*   **分配**: 2 个 Head。
*   **目标**: 捕捉用户的疲劳度、当天的复习状态趋势（User Momentum）。
*   **掩码构成 ($M^{(global)}_{i,j}$)**:
    1.  **Causal Mask**: $j > i \rightarrow -\infty$。
    2.  **No Identity Constraints**: 可以看到当前窗口内的所有历史。
    3.  **Local Context Bias**: 可以加入一个简单的相对位置衰减（非时间，而是序列索引距离），让模型更关注最近的几十次操作。

## 4. 输出层设计 (Output Heads)

经过 Transformer 层（堆叠 $N$ 层）处理后，得到最终隐藏状态 $H_L$。输出层包含两个独立的预测头。

### Head 1: 评分预测 (Ordinal Regression)
采用 **CORAL (Consistent Rank Logits)** 框架，解决评分的有序性问题。
*   **任务**: 预测评分 $y \in \{1, 2, 3, 4\}$。
*   **转换**: 将 4 分制转化为 3 个二分类子任务：$\{P(y>1), P(y>2), P(y>3)\}$。
*   **网络结构**:
    $$ \text{logits} = \text{Linear}(H_L) \rightarrow \mathbb{R}^1 $$
    $$ \text{output}_k = \sigma(\text{logits} + b_k) $$
    *   共享同一个权重向量，但拥有 3 个独立的 bias ($b_1, b_2, b_3$)。这保证了逻辑值的单调性。
*   **Loss**: $\sum_{k=1}^{3} \text{BCE}(\text{output}_k, \mathbb{I}(y_{true} > k))$。
*   **预测值解码**: $\text{Pred} = 1 + \sum_{k=1}^{3} \mathbb{I}(\text{output}_k > 0.5)$ 或直接使用概率求期望。

### Head 2: 耗时预测 (Duration Regression)
*   **任务**: 预测 $\log(\text{duration} + 1)$。
*   **网络结构**: MLP $\rightarrow \mathbb{R}^1$。
*   **Loss**: MSE Loss。

## 5. 训练与推理流程

### 训练阶段 (Training)
1.  **Batch构建**: 随机抽取用户的完整交互序列片段。
2.  **Mask生成**: 在 CPU/GPU 端预先计算好 batch 内的 `card_mask` 和 `deck_mask` 矩阵，以及 `time_diff` 矩阵。
3.  **前向传播**:
    *   计算 Time2Vec 和 Input Embedding。
    *   进入 Transformer Block，将不同的 Mask 矩阵传入对应的 Head。
    *   计算 Decay Bias: $w \cdot \exp(-\lambda \cdot \text{time\_diff})$ 动态叠加到 Logits 上。
4.  **反向传播**: 联合优化 Ordinal Loss 和 MSE Loss。

### 推理阶段 (Inference)
当需要预测某张卡片（Query Card）在当前时刻的状态时：
1.  **构造输入**:
    *   提取用户最近 $L-1$ 条历史记录。
    *   构造第 $L$ 条数据：填入 Query Card 的 `card_id`, `deck_id`, 以及计算好的 `elapsed_time`, `card_index_today` 等特征。
2.  **Mask Slice**:
    *   只需计算第 $L$ 行（最后一行）对前 $L-1$ 行的 Attention Mask。
    *   **组A**: 仅激活历史中与 Query Card `card_id` 相同的位置，并计算时间衰减。
    *   **组B**: 激活同 `deck_id` 的位置。
3.  **解码**: 获取 Head 1 和 Head 2 的输出，转换为推荐系统的优先级或复习调度安排。
