## 原始数据特征

原始数据分为两部分：复习日志 (`revlogs`) 和卡片元数据 (`cards`)。

### 复习日志 (`revlogs`)
记录了用户的交互序列。序列中的每一个时间步 `t` 包含以下核心信息：

**复习前的信息：**
- `card_id`: 卡片唯一标识符。不同用户可能存在相同的卡片标识，但并非相同的卡片。由于卡片是用户自己制作的，假设不同用户不存在相同的卡片，因此，本身也包含了用户的信息。每一卡片可视为一个问题。
- `day_offset`: 用户目前复习的总天数。
- `elapsed_days`: 本张卡片距离上次复习的天数 (-1 代表新卡片)。
- `elapsed_seconds`: 本张卡片距离上次复习的秒数 (-1 代表新卡片)。
- `state`: 卡片当时的学习状态 (新学、复习等)。

**复习后的信息：**
- `duration`: 本次复习所用时长。
- `rating`: 复习后给出的评分，(1: again, 2: hard, 3: good, 4: easy)。

### 卡片元数据 (`cards`)
- `card_id`: 卡片唯一标识符。
- `deck_id`: 卡组唯一标识符，对应“知识点 (Concept)”的概念。

## 数据处理流程 (`scr/data_processed.py`)

该脚本负责将原始的复习日志 (`revlogs`) 和卡片信息 (`cards`) 进行合并与特征工程，生成用于模型训练的序列数据。

### 1.数据读取

- 读取每个用户的复习日志 (`data/raw/revlogs/user_id={id}/data.parquet`)。
- 读取每个用户的卡片元数据 (`data/raw/cards/user_id={id}/data.parquet`)。

将复习日志与卡片数据通过 `card_id` 进行内连接 (inner merge)，将 `deck_id` 关联到复习记录中。若无对应的`cards`文件，则记为`deck_id=0`

### 2.特征构造

#### 统计特征

- `user_id`：记录对应的用户id。
- `cumulative`: 计算当日在此次复习之前的累计学习时长。通过按 `day_offset` 分组，对 `duration` 进行累加并位移（shift）实现（不包含当前行）。
- `review_count`: 对应特征 `times`。计算该卡片是第几次被复习。通过按 `card_id` 分组计数 (`cumcount() + 1`) 实现。
- `rating_level`：将评分等级转化为3维向量。例如，若原`rating`是 2，则编码为 [1, 0, 0]：
    1. 大于1？是 (1)
    2. 大于2？否 (0)
    3. 大于3？否 (0)
- `is_first_review`：bool 特征，若`elapsed_seconds=-1`，代表是新卡片，设为`True`。   
- `last_rating_on_card`：(Int) 该卡片上一次复习的评分

#### 数值变换

对所有时间相关的信息进行对数变换：$\log(x+1)$。包含：

- `log_duration`: 原`duration`
- `log_cumulative`: 原`cumulative`
- `log_elapsed_seconds`：原`elapsed_seconds`，若原先为$-1$则先置为无穷大
- `log_elapsed_days`：原`elapsed_days`，若原先为$-1$则先置为无穷大

#### 显式历史特征

- `last_rating_on_card`：该卡片上一次复习的评分，若无则设为$0$。
- `last_log_duration_on_card`: 该卡片上一次复习的耗时的对数，由`log_duration`移位获得，若无则设为无穷大。

#### Session 上下文特征

- `prev_rating`：上一次复习的评分
- `prev_log_duration`: 上一次复习的耗时的对数
- `is_same_card_as_prev`：(Bool) 本次复习的卡片是否与上一次复习是同一张。

#### 全局统计量记录

全局数值统计 (仅使用前8000个用户数据)，生成记录到`data/processed/meta.csv`中:

* `mean_log_elapsed,`, `std_log_elapsed`
* `mean_log_duration`, `std_log_duration`

用户偏好统计 (用于推理校准)，生成记录到`data/processed/user_stats.csv`中:

* `user_id`
* `user_mean_rating`, `user_std_rating`
* `user_mean_log_duration`, `user_std_log_duration`
* `total_reviews`


### 5.输出

处理后的数据保存为 `data/processed/user_id={id}.parquet`。
