# Model hyperparameters
# d_model: embedding dimension, n_heads: attention heads, depth: transformer layers
model:
  # Transformer 架构参数
  d_model: 128               # 模型维度
  n_heads: 8                 # 注意力头数
  depth: 4                   # Transformer 层数
  d_ff: null                 # 前馈网络隐藏维度，null 表示 4 * d_model
  dropout: 0.1               # Dropout 概率

  # 嵌入参数
  categorical_embed_dim: 16  # 类别特征嵌入维度
  num_rating_classes: 4      # 评分类别数 (1-4)

  # 差异化注意力头比例
  head_ratios:
    card: 0.5                # 卡片级注意力头占比
    deck: 0.25               # 卡组级注意力头占比
    global: 0.25             # 全局注意力头占比

  # 时间衰减率 (Time2Vec)
  decay_rates: [0.9, 0.99]

# 优化器参数
optimizer:
  learning_rate: 1e-4        # 初始学习率
  weight_decay: 0.01         # 权重衰减
  warmup_steps: 1000         # 预热步数

# 损失函数权重
loss:
  rating_weight: 1.0         # 评分损失权重
  duration_weight: 0.1       # 耗时损失权重
